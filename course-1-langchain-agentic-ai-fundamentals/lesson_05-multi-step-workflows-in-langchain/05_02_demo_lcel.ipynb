{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda, RunnableParallel\n",
    "from langchain_core.tracers.context import collect_runs\n",
    "from dotenv import load_dotenv\n",
    "from rich import pretty\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def pretty_off():    \n",
    "    sys.displayhook = sys.__displayhook__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "pretty.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    api_key=os.getenv(\"OPEN_AI_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a joke about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!'\u001b[0m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.invoke(\n",
    "    llm.invoke(\n",
    "        prompt.invoke(\n",
    "            {\"topic\": \"Python\"}\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runnables can be \n",
    "- executed\n",
    "    - invoke(), \n",
    "    - batch() \n",
    "    - and stream()\n",
    "- inspected,\n",
    "- and composed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnables = [prompt, llm, parser]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execute methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate\n",
      "\tINVOKE: <bound method BasePromptTemplate.invoke of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\tBATCH: <bound method Runnable.batch of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\tSTREAM: <bound method Runnable.stream of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')>\n",
      "\n",
      "ChatOpenAI\n",
      "\tINVOKE: <bound method BaseChatModel.invoke of ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7ff0407b2d20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7ff04085d400>, root_client=<openai.OpenAI object at 0x7ff040d16f30>, root_async_client=<openai.AsyncOpenAI object at 0x7ff040c268a0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))>\n",
      "\tBATCH: <bound method Runnable.batch of ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7ff0407b2d20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7ff04085d400>, root_client=<openai.OpenAI object at 0x7ff040d16f30>, root_async_client=<openai.AsyncOpenAI object at 0x7ff040c268a0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))>\n",
      "\tSTREAM: <bound method BaseChatModel.stream of ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7ff0407b2d20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7ff04085d400>, root_client=<openai.OpenAI object at 0x7ff040d16f30>, root_async_client=<openai.AsyncOpenAI object at 0x7ff040c268a0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))>\n",
      "\n",
      "StrOutputParser\n",
      "\tINVOKE: <bound method BaseOutputParser.invoke of StrOutputParser()>\n",
      "\tBATCH: <bound method Runnable.batch of StrOutputParser()>\n",
      "\tSTREAM: <bound method Runnable.stream of StrOutputParser()>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for runnable in runnables:\n",
    "    print(f\"{repr(runnable).split('(')[0]}\")\n",
    "    print(f\"\\tINVOKE: {repr(runnable.invoke)}\")\n",
    "    print(f\"\\tBATCH: {repr(runnable.batch)}\")\n",
    "    print(f\"\\tSTREAM: {repr(runnable.stream)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate\n",
      "\tINPUT: <class 'langchain_core.utils.pydantic.PromptInput'>\n",
      "\tOUTPUT: <class 'langchain_core.prompts.prompt.PromptTemplateOutput'>\n",
      "\tCONFIG: <class 'langchain_core.utils.pydantic.PromptTemplateConfig'>\n",
      "\n",
      "ChatOpenAI\n",
      "\tINPUT: <class 'langchain_openai.chat_models.base.ChatOpenAIInput'>\n",
      "\tOUTPUT: <class 'langchain_openai.chat_models.base.ChatOpenAIOutput'>\n",
      "\tCONFIG: <class 'langchain_core.utils.pydantic.ChatOpenAIConfig'>\n",
      "\n",
      "StrOutputParser\n",
      "\tINPUT: <class 'langchain_core.output_parsers.string.StrOutputParserInput'>\n",
      "\tOUTPUT: <class 'langchain_core.output_parsers.string.StrOutputParserOutput'>\n",
      "\tCONFIG: <class 'langchain_core.utils.pydantic.StrOutputParserConfig'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for runnable in runnables:\n",
    "    print(f\"{repr(runnable).split('(')[0]}\")\n",
    "    print(f\"\\tINPUT: {repr(runnable.get_input_schema())}\")\n",
    "    print(f\"\\tOUTPUT: {repr(runnable.get_output_schema())}\")\n",
    "    print(f\"\\tCONFIG: {repr(runnable.config_schema())}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with collect_runs() as run_collection:\n",
    "    result = llm.invoke(\n",
    "        \"Hello\", \n",
    "        config={\n",
    "            'run_name': 'demo_run', \n",
    "            'tags': ['demo', 'lcel'], \n",
    "            'metadata': {'lesson': 2}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mRunTree\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mid\u001b[0m=\u001b[1;35mUUID\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'429ebf2a-6f62-44ef-9e89-5a35ad8af4a4'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'demo_run'\u001b[0m,\n",
       "        \u001b[33mstart_time\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m121655\u001b[0m, \u001b[33mtzinfo\u001b[0m=\u001b[35mdatetime\u001b[0m.timezone.utc\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mrun_type\u001b[0m=\u001b[32m'llm'\u001b[0m,\n",
       "        \u001b[33mend_time\u001b[0m=\u001b[1;35mdatetime\u001b[0m\u001b[1;35m.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m31\u001b[0m, \u001b[1;36m403497\u001b[0m, \u001b[33mtzinfo\u001b[0m=\u001b[35mdatetime\u001b[0m.timezone.utc\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mextra\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'invocation_params'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model'\u001b[0m: \u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                \u001b[32m'stream'\u001b[0m: \u001b[3;91mFalse\u001b[0m,\n",
       "                \u001b[32m'n'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                \u001b[32m'temperature'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "                \u001b[32m'_type'\u001b[0m: \u001b[32m'openai-chat'\u001b[0m,\n",
       "                \u001b[32m'stop'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'options'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'stop'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'batch_size'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'lesson'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "                \u001b[32m'ls_provider'\u001b[0m: \u001b[32m'openai'\u001b[0m,\n",
       "                \u001b[32m'ls_model_name'\u001b[0m: \u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                \u001b[32m'ls_model_type'\u001b[0m: \u001b[32m'chat'\u001b[0m,\n",
       "                \u001b[32m'ls_temperature'\u001b[0m: \u001b[1;36m0.0\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33merror\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mserialized\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'lc'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'constructor'\u001b[0m,\n",
       "            \u001b[32m'id'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'langchain'\u001b[0m, \u001b[32m'chat_models'\u001b[0m, \u001b[32m'openai'\u001b[0m, \u001b[32m'ChatOpenAI'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "                \u001b[32m'temperature'\u001b[0m: \u001b[1;36m0.0\u001b[0m,\n",
       "                \u001b[32m'openai_api_key'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'lc'\u001b[0m: \u001b[1;36m1\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'secret'\u001b[0m, \u001b[32m'id'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'OPENAI_API_KEY'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'max_retries'\u001b[0m: \u001b[1;36m2\u001b[0m,\n",
       "                \u001b[32m'n'\u001b[0m: \u001b[1;36m1\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'name'\u001b[0m: \u001b[32m'ChatOpenAI'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mevents\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'name'\u001b[0m: \u001b[32m'start'\u001b[0m,\n",
       "                \u001b[32m'time'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m30\u001b[0m, \u001b[1;36m121655\u001b[0m, \u001b[33mtzinfo\u001b[0m=\u001b[35mdatetime\u001b[0m.timezone.utc\u001b[1m)\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'name'\u001b[0m: \u001b[32m'end'\u001b[0m,\n",
       "                \u001b[32m'time'\u001b[0m: \u001b[1;35mdatetime.datetime\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2025\u001b[0m, \u001b[1;36m11\u001b[0m, \u001b[1;36m9\u001b[0m, \u001b[1;36m23\u001b[0m, \u001b[1;36m17\u001b[0m, \u001b[1;36m31\u001b[0m, \u001b[1;36m403497\u001b[0m, \u001b[33mtzinfo\u001b[0m=\u001b[35mdatetime\u001b[0m.timezone.utc\u001b[1m)\u001b[0m\n",
       "            \u001b[1m}\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33minputs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'prompts'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'Human: Hello'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33moutputs\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'generations'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "                \u001b[1m[\u001b[0m\n",
       "                    \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'text'\u001b[0m: \u001b[32m'Hello! How can I assist you today?'\u001b[0m,\n",
       "                        \u001b[32m'generation_info'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m, \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                        \u001b[32m'type'\u001b[0m: \u001b[32m'ChatGeneration'\u001b[0m,\n",
       "                        \u001b[32m'message'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                            \u001b[32m'lc'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "                            \u001b[32m'type'\u001b[0m: \u001b[32m'constructor'\u001b[0m,\n",
       "                            \u001b[32m'id'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'langchain'\u001b[0m, \u001b[32m'schema'\u001b[0m, \u001b[32m'messages'\u001b[0m, \u001b[32m'AIMessage'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                            \u001b[32m'kwargs'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                \u001b[32m'content'\u001b[0m: \u001b[32m'Hello! How can I assist you today?'\u001b[0m,\n",
       "                                \u001b[32m'additional_kwargs'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'refusal'\u001b[0m: \u001b[3;35mNone\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                                \u001b[32m'response_metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                    \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                        \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "                                        \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                                        \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "                                        \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                            \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                                            \u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                                            \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                                            \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "                                        \u001b[1m}\u001b[0m,\n",
       "                                        \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                            \u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                                            \u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "                                        \u001b[1m}\u001b[0m\n",
       "                                    \u001b[1m}\u001b[0m,\n",
       "                                    \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                                    \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_560af6e559'\u001b[0m,\n",
       "                                    \u001b[32m'finish_reason'\u001b[0m: \u001b[32m'stop'\u001b[0m,\n",
       "                                    \u001b[32m'logprobs'\u001b[0m: \u001b[3;35mNone\u001b[0m\n",
       "                                \u001b[1m}\u001b[0m,\n",
       "                                \u001b[32m'type'\u001b[0m: \u001b[32m'ai'\u001b[0m,\n",
       "                                \u001b[32m'id'\u001b[0m: \u001b[32m'run--429ebf2a-6f62-44ef-9e89-5a35ad8af4a4-0'\u001b[0m,\n",
       "                                \u001b[32m'usage_metadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                                    \u001b[32m'input_tokens'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                                    \u001b[32m'output_tokens'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "                                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "                                    \u001b[32m'input_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cache_read'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                                    \u001b[32m'output_token_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'reasoning'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                                \u001b[1m}\u001b[0m,\n",
       "                                \u001b[32m'tool_calls'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "                                \u001b[32m'invalid_tool_calls'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "                            \u001b[1m}\u001b[0m\n",
       "                        \u001b[1m}\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m\n",
       "                \u001b[1m]\u001b[0m\n",
       "            \u001b[1m]\u001b[0m,\n",
       "            \u001b[32m'llm_output'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'token_usage'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                    \u001b[32m'completion_tokens'\u001b[0m: \u001b[1;36m9\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens'\u001b[0m: \u001b[1;36m8\u001b[0m,\n",
       "                    \u001b[32m'total_tokens'\u001b[0m: \u001b[1;36m17\u001b[0m,\n",
       "                    \u001b[32m'completion_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                        \u001b[32m'accepted_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'reasoning_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m,\n",
       "                        \u001b[32m'rejected_prediction_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "                    \u001b[1m}\u001b[0m,\n",
       "                    \u001b[32m'prompt_tokens_details'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'audio_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m, \u001b[32m'cached_tokens'\u001b[0m: \u001b[1;36m0\u001b[0m\u001b[1m}\u001b[0m\n",
       "                \u001b[1m}\u001b[0m,\n",
       "                \u001b[32m'model_name'\u001b[0m: \u001b[32m'gpt-4o-mini-2024-07-18'\u001b[0m,\n",
       "                \u001b[32m'system_fingerprint'\u001b[0m: \u001b[32m'fp_560af6e559'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'run'\u001b[0m: \u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'LLMResult'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mreference_example_id\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mparent_run_id\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtags\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'demo'\u001b[0m, \u001b[32m'lcel'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mattachments\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mchild_runs\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33msession_name\u001b[0m=\u001b[32m'default'\u001b[0m,\n",
       "        \u001b[33msession_id\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mdotted_order\u001b[0m=\u001b[32m'20251109T231730121655Z429ebf2a-6f62-44ef-9e89-5a35ad8af4a4'\u001b[0m,\n",
       "        \u001b[33mtrace_id\u001b[0m=\u001b[1;35mUUID\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'429ebf2a-6f62-44ef-9e89-5a35ad8af4a4'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mdangerously_allow_filesystem\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mreplicas\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_collection.traced_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compose Runnables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableSequence(prompt, llm, parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'langchain_core.runnables.base.RunnableSequence'\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!'\u001b[0m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"Python\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do Python programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!"
     ]
    }
   ],
   "source": [
    "for chunk in chain.stream({\"topic\": \"Python\"}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs!'\u001b[0m,\n",
       "    \u001b[32m'Why did the data break up with the database?\\n\\nBecause it found too many \"issues\" in their relationship!'\u001b[0m,\n",
       "    \u001b[32m'Why did the neural network break up with the decision tree?\\n\\nBecause it found someone with more layers!'\u001b[0m\n",
       "\u001b[1m]\u001b[0m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\n",
    "    {\"topic\": \"Python\"},\n",
    "    {\"topic\": \"Data\"},\n",
    "    {\"topic\": \"Machine Learning\"},\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +------------+       \n",
      "      | ChatOpenAI |       \n",
      "      +------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn any function into a runnable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double(x:int)->int:\n",
    "    return 2*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;36m4\u001b[0m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runnable = RunnableLambda(double)\n",
    "runnable.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parallel Runnables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_chain = RunnableParallel(\n",
    "    double=RunnableLambda(lambda x: x * 2),\n",
    "    triple=RunnableLambda(lambda x: x * 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'double'\u001b[0m: \u001b[1;36m6\u001b[0m, \u001b[32m'triple'\u001b[0m: \u001b[1;36m9\u001b[0m\u001b[1m}\u001b[0m"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+   \n",
      "| Parallel<double,triple>Input |   \n",
      "+------------------------------+   \n",
      "           **        **            \n",
      "         **            **          \n",
      "        *                *         \n",
      "  +--------+          +--------+   \n",
      "  | Lambda |          | Lambda |   \n",
      "  +--------+          +--------+   \n",
      "           **        **            \n",
      "             **    **              \n",
      "               *  *                \n",
      "+-------------------------------+  \n",
      "| Parallel<double,triple>Output |  \n",
      "+-------------------------------+  \n"
     ]
    }
   ],
   "source": [
    "parallel_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mPromptTemplate\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33minput_variables\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'topic'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33minput_types\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mpartial_variables\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mtemplate\u001b[0m=\u001b[32m'Tell me a joke about \u001b[0m\u001b[32m{\u001b[0m\u001b[32mtopic\u001b[0m\u001b[32m}\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "\u001b[1;35mChatOpenAI\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mclient\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mopenai.resources.chat.completions.completions.Completions\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7ff0407b2d20\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33masync_client\u001b[0m\u001b[39m=<openai.resources.chat.completions.completions.AsyncCompletions object at \u001b[0m\u001b[1;36m0x7ff04085d400\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mroot_client\u001b[0m\u001b[39m=<openai.OpenAI object at \u001b[0m\u001b[1;36m0x7ff040d16f30\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mroot_async_client\u001b[0m\u001b[39m=<openai.AsyncOpenAI object at \u001b[0m\u001b[1;36m0x7ff040c268a0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "    \u001b[33mmodel_name\u001b[0m=\u001b[32m'gpt-4o-mini'\u001b[0m,\n",
       "    \u001b[33mtemperature\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "    \u001b[33mmodel_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mopenai_api_key\u001b[0m=\u001b[1;35mSecretStr\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'**********'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mStrOutputParser\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7ff0407b2d20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7ff04085d400>, root_client=<openai.OpenAI object at 0x7ff040d16f30>, root_async_client=<openai.AsyncOpenAI object at 0x7ff040c268a0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "pretty_off()\n",
    "\n",
    "chain = RunnableSequence(prompt, llm, parser)\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7ff0407b2d20>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7ff04085d400>, root_client=<openai.OpenAI object at 0x7ff040d16f30>, root_async_client=<openai.AsyncOpenAI object at 0x7ff040c268a0>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "pretty_off()\n",
    "\n",
    "prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m'Why did the computer go to therapy?\\n\\nBecause it had too many bytes from its past!'\u001b[0m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\"topic\": \"computer\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nd901-agentic-ai-with-langchain-and-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
