{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise - Knowledge Base Agent - STARTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, youâ€™ll implement and evaluate a RAG (Retrieval-Augmented Generation) pipeline, using RAGAS metrics and MLflow for logging the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to create a LangGraph Workflow that includes:\n",
    "\n",
    "- A RAG pipeline for information retrieval.\n",
    "- An LLM-based judge for evaluation.\n",
    "- RAGAS metrics for quality assessment.\n",
    "- MLflow logging for observability.\n",
    "\n",
    "The workflow should:\n",
    "\n",
    "- Retrieve, augment, and generate answers.\n",
    "- Evaluate the answers using RAGAS.\n",
    "- Log performance metrics in MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import the necessary libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import log_params, log_metrics\n",
    "from typing import List, Dict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate Chat Model with your API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to connect with OpenAI, you need to instantiate an ChatOpenAI client passing your OpenAI key.\n",
    "\n",
    "You can pass the `api_key` argument directly.\n",
    "```python\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    api_key=\"voc-\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Instantiate your chat model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    api_key = \"YOUR_API_KEY_HERE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Instantiate your llm as judge model\n",
    "# This will evaluate the responses\n",
    "llm_judge = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    api_key = \"YOUR_API_KEY_HERE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Instantiate your embeddings model\n",
    "embeddings_fn = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    "    api_key = \"YOUR_API_KEY_HERE\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"udacity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"l4_exercise_02\") as run:\n",
    "    log_params(\n",
    "        {\n",
    "            \"embeddings_model\":embeddings_fn.model,\n",
    "            \"llm_model\": llm.model_name,\n",
    "            \"llm_judge_model\": llm_judge.model_name,\n",
    "        }\n",
    "    )\n",
    "    print(run.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mflow_client = mlflow.tracking.MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mflow_client.get_run(mlflow_run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vector store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"udacity\",\n",
    "    embedding_function=embeddings_fn\n",
    ")\n",
    "\n",
    "# Load and process PDF documents\n",
    "file_path = \"compact-guide-to-large-language-models.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page)\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "# Store document chunks in the vector database\n",
    "_ = vector_store.add_documents(documents=all_splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define State Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a State Schema for managing:\n",
    "\n",
    "- MLFlow Run id\n",
    "- User query\n",
    "- Ground Truth\n",
    "- Retrieved documents\n",
    "- Generated answer\n",
    "- Evaluation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run_id(str), ground_truth(str), evaluation(Dict),vquestion(str), documents(List) and answer(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Create your state schema\n",
    "class State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent should:\n",
    "- fetch relevant document chunks based on the user query\n",
    "- combine the retrieved documents and use them as context\n",
    "- invoke the LLM to generate a response\n",
    "- evaluate the pipeline based on the ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    question = state[\"question\"]\n",
    "    retrieved_docs = vector_store.similarity_search(question)\n",
    "    return {\"documents\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "        (\"human\", \"Use the following pieces of retrieved context to answer the question. \"\n",
    "                \"If you don't know the answer, just say that you don't know. \" \n",
    "                \"Use three sentences maximum and keep the answer concise. \"\n",
    "                \"\\n# Question: \\n-> {question} \"\n",
    "                \"\\n# Context: \\n-> {context} \"\n",
    "                \"\\n# Answer: \"),\n",
    "    ])\n",
    "\n",
    "    messages = template.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "    ).to_messages()\n",
    "\n",
    "    return {\"messages\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    ai_message = llm.invoke(state[\"messages\"])\n",
    "    return {\"answer\": ai_message.content, \"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag(state: State):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    answer = state[\"answer\"]\n",
    "    ground_truth = state[\"ground_truth\"]\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"question\": [question],\n",
    "            \"answer\": [answer],\n",
    "            \"contexts\": [[doc.page_content for doc in documents]],\n",
    "            \"ground_truth\": [ground_truth]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    evaluation_results = evaluate(\n",
    "        dataset=dataset,\n",
    "        llm=llm_judge\n",
    "    )\n",
    "    print(evaluation_results)\n",
    "\n",
    "    # TODO - Log metrics in MLflow\n",
    "    # The evaluation_results output value is a list\n",
    "    # Example: evaluation_results[\"faithfulness\"][0]\n",
    "    with mlflow.start_run(state[\"run_id\"]):\n",
    "        log_metrics({\n",
    "            \"faithfulness\": \"\",\n",
    "            \"context_precision\": \"\",\n",
    "            \"context_recall\": \"\",\n",
    "            \"answer_relevancy\": \"\",\n",
    "        })\n",
    "\n",
    "    return {\"evaluation\": evaluation_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build the LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - add all the nodes and edges\n",
    "workflow = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = workflow.compile()\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Invoke the Workflow with a Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [\n",
    "    {\n",
    "        \"question\": \"What are Open source models?\",\n",
    "        \"ground_truth\": \"Open-source models are AI or machine learning \" \n",
    "                        \"models whose code, architecture, and in some cases, \" \n",
    "                        \"training data and weights, are publicly available for \" \n",
    "                        \"use, modification, and distribution. They enable \" \n",
    "                        \"collaboration, transparency, and innovation by allowing \" \n",
    "                        \"developers to fine-tune, deploy, or improve them without \" \n",
    "                        \"proprietary restrictions.\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = graph.invoke(\n",
    "    {\n",
    "        \"question\": reference[0][\"question\"],\n",
    "        \"ground_truth\": reference[0][\"ground_truth\"],\n",
    "        \"run_id\": mlflow_run_id\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inspect in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Get MLFlow Run with .get_run()\n",
    "mflow_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you understood how it works, experiment with new things.\n",
    "\n",
    "- Change RAG parameters: embedding model, chunk_size, chunk_overlap...\n",
    "- Create multiple runs\n",
    "- Improve your reference with more questions and ground_truth answers\n",
    "- Use the results to understand what are the best parameters\n",
    "- Create an Agent that picks the best combination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
